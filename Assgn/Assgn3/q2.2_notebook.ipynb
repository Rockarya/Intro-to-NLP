{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://www.guru99.com/seq2seq-model.html\nfrom __future__ import unicode_literals, print_function, division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport string\nimport spacy\nimport os\nimport re\nimport random\nimport joblib\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:41:00.365712Z","iopub.execute_input":"2022-04-12T19:41:00.366069Z","iopub.status.idle":"2022-04-12T19:41:12.548938Z","shell.execute_reply.started":"2022-04-12T19:41:00.365947Z","shell.execute_reply":"2022-04-12T19:41:12.548182Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Declaring out Model\nclass WordLSTM(nn.Module):\n    \n    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n        super().__init__()\n\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        \n        self.emb_layer = nn.Embedding(vocab_size, 200)\n\n        ## define the LSTM\n        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        ## define a dropout layer\n        self.dropout = nn.Dropout(drop_prob)\n        \n        ## define the fully-connected layer\n        self.fc = nn.Linear(n_hidden, vocab_size)      \n    \n    def forward(self, x, hidden):\n        ''' Forward pass through the network. \n            These inputs are x, and the hidden/cell state `hidden`. '''\n\n        ## pass input through embedding layer\n        embedded = self.emb_layer(x)     \n        \n        ## Get the outputs and the new hidden state from the lstm\n        lstm_output, hidden = self.lstm(embedded, hidden)\n        \n        ## pass through a dropout layer\n        out = self.dropout(lstm_output)\n        \n        #out = out.contiguous().view(-1, self.n_hidden) \n        out = out.reshape(-1, self.n_hidden) \n\n        ## put \"out\" through the fully-connected layer\n        out = self.fc(out)\n\n        # return the final output and the hidden state\n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n\n        # if GPU is available\n        if (torch.cuda.is_available()):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        \n        # if GPU is not available\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        \n        return hidden\n\nvocab_size = 17706\nfrench = WordLSTM()\n# push the model to GPU (avoid it if you are not using the GPU)\nfrench.cuda()    \n\nmodel = torch.load('../input/lang-models/q1_french.pt', map_location=device)\nfrench.load_state_dict(model)\nfrench.eval()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:41:12.550512Z","iopub.execute_input":"2022-04-12T19:41:12.550746Z","iopub.status.idle":"2022-04-12T19:41:21.575612Z","shell.execute_reply.started":"2022-04-12T19:41:12.550712Z","shell.execute_reply":"2022-04-12T19:41:21.574906Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"vocab_size = 9330\nenglish = WordLSTM()\n# push the model to GPU (avoid it if you are not using the GPU)\nenglish.cuda()    \n\nmodel = torch.load('../input/lang-models/q1_english.pt', map_location=device)\nenglish.load_state_dict(model)\nenglish.eval()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:41:21.577171Z","iopub.execute_input":"2022-04-12T19:41:21.577443Z","iopub.status.idle":"2022-04-12T19:41:21.864881Z","shell.execute_reply.started":"2022-04-12T19:41:21.577407Z","shell.execute_reply":"2022-04-12T19:41:21.864083Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# HYPER PARAMETERS  \nthreshold = 2\n\nepochs = 5\n\n# sentence length allowed\nmax_length = 25\n\nteacher_forcing_ratio = 0.5\n\nSOS_token = 0\nEOS_token = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:41:21.867063Z","iopub.execute_input":"2022-04-12T19:41:21.867330Z","iopub.status.idle":"2022-04-12T19:41:21.871481Z","shell.execute_reply.started":"2022-04-12T19:41:21.867295Z","shell.execute_reply":"2022-04-12T19:41:21.870664Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# download spacy tokenizer for english and french language\n!python3 -m spacy download en_core_web_sm\n!python3 -m spacy download fr_core_news_sm\n\nspacy_fr = spacy.load('fr_core_news_sm')\nspacy_en = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:41:21.872933Z","iopub.execute_input":"2022-04-12T19:41:21.873316Z","iopub.status.idle":"2022-04-12T19:42:14.744255Z","shell.execute_reply.started":"2022-04-12T19:41:21.873278Z","shell.execute_reply":"2022-04-12T19:42:14.743461Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# read file\nenglish_file_name = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/train.en'\nf = open(english_file_name)\nen_text = f.read()\nf.close()\n\nfrench_file_name = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/train.fr'\nf = open(french_file_name)\nfr_text = f.read()\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:06.406799Z","iopub.execute_input":"2022-04-12T19:46:06.407079Z","iopub.status.idle":"2022-04-12T19:46:06.570016Z","shell.execute_reply.started":"2022-04-12T19:46:06.407027Z","shell.execute_reply":"2022-04-12T19:46:06.569282Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# tokenize french and english language sentences\n\nen_text = en_text.lower()\nen_sentences = nltk.tokenize.sent_tokenize(en_text)\n\nfr_text = fr_text.lower()\nfr_sentences = nltk.tokenize.sent_tokenize(fr_text)\n\nsource = []\ntarget = []\nfor en_sent, fr_sent in zip(en_sentences, fr_sentences):\n    source.append([tok.text for tok in spacy_en.tokenizer(en_sent)])\n    target.append([tok.text for tok in spacy_fr.tokenizer(fr_sent)])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:06.886495Z","iopub.execute_input":"2022-04-12T19:46:06.886726Z","iopub.status.idle":"2022-04-12T19:46:20.540720Z","shell.execute_reply.started":"2022-04-12T19:46:06.886700Z","shell.execute_reply":"2022-04-12T19:46:20.539865Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# calculating frequency and assigning unknown to words having frequcny less than threshold\nen_freq = {}\nfr_freq = {}\nfor en_lst, fr_lst in zip(source, target):\n    for en_token in en_lst:\n        if en_freq.get(en_token) == None:\n            en_freq[en_token] = 1\n        else:\n            en_freq[en_token] += 1\n            \n    for fr_token in fr_lst:\n        if fr_freq.get(fr_token) == None:\n            fr_freq[fr_token] = 1\n        else:\n            fr_freq[fr_token] += 1\n            \n            \nen_sentences = []\nfr_sentences = []\nfor en_lst, fr_lst in zip(source, target):\n    en_sent = []\n    for word in en_lst:\n        if word not in string.punctuation:\n            if en_freq[word] >= threshold:\n                en_sent.append(word)\n            else:\n                en_sent.append('unk')\n                \n    en_sentences.append(en_sent)\n    \n    fr_sent = []\n    for word in fr_lst:\n        if word not in string.punctuation:\n            if fr_freq[word] >= threshold:\n                fr_sent.append(word)\n            else:\n                fr_sent.append('unk')\n        \n    fr_sentences.append(fr_sent)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:20.542428Z","iopub.execute_input":"2022-04-12T19:46:20.542659Z","iopub.status.idle":"2022-04-12T19:46:21.567947Z","shell.execute_reply.started":"2022-04-12T19:46:20.542627Z","shell.execute_reply":"2022-04-12T19:46:21.567044Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# storing unique words in vocabulary and creatinf sentecne of max_length and appending eos,sos tokens and creating word2index and index2word dicts\nen_vocab = set()\nfr_vocab = set()\npads = ['sos', 'eos', 'unk']\nfor p in pads:\n    en_vocab.add(p)\n    fr_vocab.add(p)\n\n# here we are storing words in src and trg\nsrc = []\ntrg = []\nfor en_sent, fr_sent in zip(en_sentences, fr_sentences):\n    en_lst = []\n    for i in range(min(max_length, len(en_sent))):\n        en_lst.append(en_sent[i])\n        en_vocab.add(en_sent[i])\n    \n    fr_lst = []\n    for i in range(min(max_length, len(fr_sent))):\n        fr_lst.append(fr_sent[i])\n        fr_vocab.add(fr_sent[i])\n    \n    src.append(en_lst)\n    trg.append(fr_lst)\n    \n\nen_token2index = {}\nen_index2token = {}\nfor cnt,token in enumerate(en_vocab):\n    en_token2index[token] = cnt\n    en_index2token[cnt] = token\n\nfr_token2index = {}\nfr_index2token = {}\nfor cnt,token in enumerate(fr_vocab):\n    fr_token2index[token] = cnt\n    fr_index2token[cnt] = token","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:21.569280Z","iopub.execute_input":"2022-04-12T19:46:21.569734Z","iopub.status.idle":"2022-04-12T19:46:22.034770Z","shell.execute_reply.started":"2022-04-12T19:46:21.569693Z","shell.execute_reply":"2022-04-12T19:46:22.034039Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"source = []\ntarget = []\npairs = []\nfor en_sent, fr_sent in zip(src, trg):\n    en_lst = []\n    for en_token in en_sent:\n        en_lst.append(en_token2index[en_token])\n    \n    fr_lst = []\n    for fr_token in fr_sent:\n        fr_lst.append(fr_token2index[fr_token])\n    \n    source.append(en_lst)\n    target.append(fr_lst)\n    pairs.append([en_lst,fr_lst])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:22.037762Z","iopub.execute_input":"2022-04-12T19:46:22.038306Z","iopub.status.idle":"2022-04-12T19:46:22.337273Z","shell.execute_reply.started":"2022-04-12T19:46:22.038257Z","shell.execute_reply":"2022-04-12T19:46:22.336473Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(len(src),len(trg),len(pairs))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:22.338661Z","iopub.execute_input":"2022-04-12T19:46:22.339423Z","iopub.status.idle":"2022-04-12T19:46:22.344914Z","shell.execute_reply.started":"2022-04-12T19:46:22.339384Z","shell.execute_reply":"2022-04-12T19:46:22.344218Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# ENCODER DECODER SEQ2SEQ\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n        super(Encoder, self).__init__()\n\n        #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n        self.input_dim = input_dim\n        self.embbed_dim = embbed_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        #initialize the embedding layer with input and embbed dimention\n        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n        #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n        #set the number of gru layers\n        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n              \n    def forward(self, src):\n\n        embedded = self.embedding(src).view(1,1,-1)\n        outputs, hidden = self.gru(embedded)\n        return outputs, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n        super(Decoder, self).__init__()\n\n        #set the encoder output dimension, embed dimension, hidden dimension, and number of layers \n        self.embbed_dim = embbed_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n\n        # initialize every layer with the appropriate dimension. For the decoder layer, it will consist of an embedding, GRU, a Linear layer and a Log softmax activation function.\n        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n        self.out = nn.Linear(self.hidden_dim, output_dim)\n        self.softmax = nn.LogSoftmax(dim=1)\n      \n    def forward(self, input, hidden):\n        # reshape the input to (1, batch_size)\n        input = input.view(1, -1)\n        embedded = F.relu(self.embedding(input))\n        output, hidden = self.gru(embedded, hidden)       \n        prediction = self.softmax(self.out(output[0]))\n\n        return prediction, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device, MAX_LENGTH=max_length):\n        super().__init__()\n      \n        #initialize the encoder and decoder\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        \n        input_length = source.size(0) #get the input length (number of words in sentence)\n        batch_size = target.shape[1] \n        target_length = target.shape[0]\n        vocab_size = self.decoder.output_dim\n\n        #initialize a variable to hold the predicted outputs\n        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n\n        #encode every word in a sentence\n        for i in range(input_length):\n            encoder_output, encoder_hidden = self.encoder(source[i])\n\n        #use the encoder’s hidden layer as the decoder hidden\n        decoder_hidden = encoder_hidden.to(device)\n\n        #add a token before the first predicted word\n        decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n\n        #topk is used to get the top K value over a list\n        #predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n\n        for t in range(target_length):   \n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n            outputs[t] = decoder_output\n            teacher_force = random.random() < teacher_forcing_ratio\n            topv, topi = decoder_output.topk(1)\n            input = (target[t] if teacher_force else topi)\n            if(teacher_force == False and input.item() == EOS_token):\n                break\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:42:15.666823Z","iopub.execute_input":"2022-04-12T19:42:15.667145Z","iopub.status.idle":"2022-04-12T19:42:15.686523Z","shell.execute_reply.started":"2022-04-12T19:42:15.667107Z","shell.execute_reply":"2022-04-12T19:42:15.685704Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def calcError(model, input_tensor, target_tensor, model_optimizer, criterion):\n    model_optimizer.zero_grad()\n\n    loss = 0\n    epoch_loss = 0\n\n    output = model(input_tensor, target_tensor)\n\n    num_iter = output.size(0)\n                \n    #calculate the loss from a predicted sentence with the expected result\n    for ot in range(num_iter):\n        loss += criterion(output[ot], target_tensor[ot])\n\n    loss.backward()\n    model_optimizer.step()\n    epoch_loss = loss.item() / num_iter\n    return epoch_loss\n\n\ndef trainModel(model, pairs, num_iteration):\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n    criterion = nn.NLLLoss()\n    total_loss_iterations = 0\n\n    for iter in range(1, num_iteration+1):\n        training_pair = pairs[iter - 1]\n        input_tensor = torch.tensor(training_pair[0], dtype=torch.long, device=device).view(-1, 1)\n        target_tensor = torch.tensor(training_pair[1], dtype=torch.long, device=device).view(-1, 1)\n        \n        try:\n            loss = calcError(model, input_tensor, target_tensor, optimizer, criterion)\n        except:\n            do_nothing = 1\n        \n        total_loss_iterations += loss\n\n        if iter % 300 == 0:\n            avarage_loss= total_loss_iterations / 300\n            total_loss_iterations = 0\n            print('Iteration Number: %d     Avg Loss: %.4f' % (iter, avarage_loss))\n          \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:42:15.687731Z","iopub.execute_input":"2022-04-12T19:42:15.688470Z","iopub.status.idle":"2022-04-12T19:42:15.700810Z","shell.execute_reply.started":"2022-04-12T19:42:15.688427Z","shell.execute_reply":"2022-04-12T19:42:15.699996Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"input_size = len(en_vocab)\noutput_size = len(fr_vocab)\nprint('Input : {} Output : {}'.format(input_size, output_size))\n\nembed_size = 256\nhidden_size = 512\nnum_layers = 1\n# total number of senetences\nnum_iteration = len(pairs)\n\n#create encoder-decoder model\nencoder = Encoder(input_size, hidden_size, embed_size, num_layers)\ndecoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n\nmodel = Seq2Seq(encoder, decoder, device).to(device)\n\nencoder.state_dict()['gru.weight_ih_l0'] = english.state_dict()['lstm.weight_hh_l3']\nencoder.state_dict()['gru.weight_ih_l0'] = french.state_dict()['lstm.weight_hh_l3']\n\n#print model \nprint(encoder)\nprint(decoder)\n\nfor ep in range(epochs):\n    model = trainModel(model, pairs, num_iteration)\n    print('Epoch {} done'.format(ep+1))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:42:15.702007Z","iopub.execute_input":"2022-04-12T19:42:15.702915Z","iopub.status.idle":"2022-04-12T19:45:21.876264Z","shell.execute_reply.started":"2022-04-12T19:42:15.702872Z","shell.execute_reply":"2022-04-12T19:45:21.875481Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'q2.1_train_french.pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:45:21.877535Z","iopub.execute_input":"2022-04-12T19:45:21.877784Z","iopub.status.idle":"2022-04-12T19:45:21.906902Z","shell.execute_reply.started":"2022-04-12T19:45:21.877739Z","shell.execute_reply":"2022-04-12T19:45:21.906185Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def french_sentence(input_sentence):\n    input_tensor = torch.tensor(input_sentence, dtype=torch.long, device=device).view(-1, 1)\n    target = np.zeros(len(input_sentence))\n    output_tensor = torch.tensor(target, dtype=torch.long, device=device).view(-1, 1)\n    # setting the teacher forcing ratio to be 0.0, so we get only the words predicted by the model     \n    output = model(input_tensor, output_tensor,0.0)\n    num_iter = output.size(0)\n    decoded_words = []\n    for ot in range(output.size(0)):\n        topv, topi = output[ot].topk(1)\n        # print(topi)\n\n        if topi[0].item() == EOS_token:\n            decoded_words.append('<EOS>')\n            break\n        else:\n            decoded_words.append(fr_index2token[topi[0].item()])\n\n    return decoded_words","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:22.346271Z","iopub.execute_input":"2022-04-12T19:46:22.346753Z","iopub.status.idle":"2022-04-12T19:46:22.355695Z","shell.execute_reply.started":"2022-04-12T19:46:22.346719Z","shell.execute_reply":"2022-04-12T19:46:22.355025Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def write_in_file(file_name, trg, pairs):\n    corpus_references = []\n    corpus_candidates = []\n\n    sentence_score = []\n    length = len(trg)\n    french = []\n    for i in range(length):\n        reference = trg[i]\n        french.append(french_sentence(pairs[i][0]))\n        candidate = french[i]\n        try:\n            score = sentence_bleu(reference, candidate)\n        except:\n            score = 0.0\n        sentence_score.append(score)\n        corpus_references.append(reference)\n        corpus_candidates.append(candidate)\n\n    corpus_score = corpus_bleu(corpus_references, corpus_candidates)  \n    \n    to_write = ''\n    to_write += (str(corpus_score) + '\\n')\n    \n    for cnt, lst in enumerate(french):\n        sent = \" \".join(lst)\n        to_write += sent\n        to_write += '     '\n        to_write += str(sentence_score[cnt])\n        to_write += '\\n'\n        \n    file = open(file_name, 'w')\n    file.write(to_write)\n    file.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:22.356547Z","iopub.execute_input":"2022-04-12T19:46:22.357921Z","iopub.status.idle":"2022-04-12T19:46:22.369497Z","shell.execute_reply.started":"2022-04-12T19:46:22.357883Z","shell.execute_reply":"2022-04-12T19:46:22.368809Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"write_in_file('2019101056_MT2_train.txt',trg,pairs)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:22.372523Z","iopub.execute_input":"2022-04-12T19:46:22.372745Z","iopub.status.idle":"2022-04-12T19:46:22.920053Z","shell.execute_reply.started":"2022-04-12T19:46:22.372717Z","shell.execute_reply":"2022-04-12T19:46:22.918848Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def generate_for_test(english_filename, french_filename):\n    # read file\n    f = open(english_file_name)\n    en_text = f.read()\n    f.close()\n\n    f = open(french_file_name)\n    fr_text = f.read()\n    f.close()\n\n    # tokenize french and english language sentences\n    en_text = en_text.lower()\n    en_sentences = nltk.tokenize.sent_tokenize(en_text)\n\n    fr_text = fr_text.lower()\n    fr_sentences = nltk.tokenize.sent_tokenize(fr_text)\n\n    source = []\n    target = []\n    for en_sent, fr_sent in zip(en_sentences, fr_sentences):\n        source.append([tok.text for tok in spacy_en.tokenizer(en_sent)])\n        target.append([tok.text for tok in spacy_fr.tokenizer(fr_sent)])\n\n\n    trg = []\n    pairs = []\n    for en_sent, fr_sent in zip(source,target):\n        en_lst = []\n        for token in en_sent:\n            if en_token2index.get(token) == None:\n                en_lst.append(en_token2index['unk'])\n            else:\n                en_lst.append(en_token2index[token])\n\n\n        fr_lst = []\n        fr_tokens = []\n        for token in fr_sent:\n            if fr_token2index.get(token) == None:\n                fr_lst.append(fr_token2index['unk'])\n                fr_tokens.append('unk')\n            else:\n                fr_lst.append(fr_token2index[token])\n                fr_tokens.append(token)\n\n        trg.append(fr_tokens)\n        pairs.append([en_lst,fr_lst])\n        \n    return trg, pairs","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:46:22.921261Z","iopub.status.idle":"2022-04-12T19:46:22.922254Z","shell.execute_reply.started":"2022-04-12T19:46:22.921984Z","shell.execute_reply":"2022-04-12T19:46:22.922011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FOR GENERATING FILE ON TEST DATASET\nenglish_filename = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/test.en'\nfrench_filename = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/test.fr'\ntrg, pairs = generate_for_test(english_filename, french_filename)\nwrite_in_file('2019101056_MT2_test.txt',trg,pairs)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T11:40:31.037715Z","iopub.status.idle":"2022-04-12T11:40:31.038481Z","shell.execute_reply.started":"2022-04-12T11:40:31.038216Z","shell.execute_reply":"2022-04-12T11:40:31.038265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}