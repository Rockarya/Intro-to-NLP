{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# RUN ALL KRNA HAI\nfrom __future__ import unicode_literals, print_function, division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport string\nimport spacy\nimport os\nimport re\nimport random\nimport joblib\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:22:57.867520Z","iopub.execute_input":"2022-04-12T17:22:57.867909Z","iopub.status.idle":"2022-04-12T17:23:09.181214Z","shell.execute_reply.started":"2022-04-12T17:22:57.867800Z","shell.execute_reply":"2022-04-12T17:23:09.180471Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# HYPER PARAMETERS  \nthreshold = 2\n\nepochs = 5\n\n# sentence length allowed\nmax_length = 25\n\nteacher_forcing_ratio = 0.5\n\nSOS_token = 0\nEOS_token = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:23:09.183017Z","iopub.execute_input":"2022-04-12T17:23:09.183260Z","iopub.status.idle":"2022-04-12T17:23:09.187944Z","shell.execute_reply.started":"2022-04-12T17:23:09.183226Z","shell.execute_reply":"2022-04-12T17:23:09.187286Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# download spacy tokenizer for english and french language\n!python3 -m spacy download en_core_web_sm\n!python3 -m spacy download fr_core_news_sm\n\nspacy_fr = spacy.load('fr_core_news_sm')\nspacy_en = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:23:09.189050Z","iopub.execute_input":"2022-04-12T17:23:09.189771Z","iopub.status.idle":"2022-04-12T17:23:59.800645Z","shell.execute_reply.started":"2022-04-12T17:23:09.189732Z","shell.execute_reply":"2022-04-12T17:23:59.799868Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.2.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n     |████████████████████████████████| 13.9 MB 12.0 MB/s            \n\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.2.0) (3.2.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.5.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\nRequirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\nCollecting typing-extensions<4.0.0.0,>=3.7.4\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\nRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.11.3)\nInstalling collected packages: typing-extensions\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Uninstalling typing-extensions-4.1.1:\n      Successfully uninstalled typing-extensions-4.1.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nexplainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\ntensorflow 2.6.2 requires numpy~=1.19.2, but you have numpy 1.20.3 which is incompatible.\ntensorflow 2.6.2 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\ntensorflow 2.6.2 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\ntensorflow 2.6.2 requires wrapt~=1.12.1, but you have wrapt 1.13.3 which is incompatible.\ntensorflow-transform 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntensorflow-transform 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.3 which is incompatible.\ntensorflow-transform 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\ntensorflow-transform 1.5.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<2.8,>=1.15.2, but you have tensorflow 2.6.2 which is incompatible.\ntensorflow-serving-api 2.7.0 requires tensorflow<3,>=2.7.0, but you have tensorflow 2.6.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.11.3 which is incompatible.\napache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\napache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.1 which is incompatible.\naioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.1.2 requires botocore<1.23.25,>=1.23.24, but you have botocore 1.24.20 which is incompatible.\u001b[0m\nSuccessfully installed typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\nCollecting fr-core-news-sm==3.2.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.2.0/fr_core_news_sm-3.2.0-py3-none-any.whl (17.4 MB)\n     |████████████████████████████████| 17.4 MB 35.8 MB/s            \n\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from fr-core-news-sm==3.2.0) (3.2.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.8.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (59.5.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.9.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.3)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.20.3)\nRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.10.0.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.4.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (21.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (4.62.3)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.6.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.9)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\nRequirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (8.0.15)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.26.0)\nRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.4.2)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.7.6)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.6)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (5.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.26.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (8.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (4.11.3)\nInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"# read file\nenglish_file_name = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/train.en'\nf = open(english_file_name)\nen_text = f.read()\nf.close()\n\nfrench_file_name = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/train.fr'\nf = open(french_file_name)\nfr_text = f.read()\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:23:59.803346Z","iopub.execute_input":"2022-04-12T17:23:59.803787Z","iopub.status.idle":"2022-04-12T17:23:59.965841Z","shell.execute_reply.started":"2022-04-12T17:23:59.803749Z","shell.execute_reply":"2022-04-12T17:23:59.965113Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# tokenize french and english language sentences\n\nen_text = en_text.lower()\nen_sentences = nltk.tokenize.sent_tokenize(en_text)\n\nfr_text = fr_text.lower()\nfr_sentences = nltk.tokenize.sent_tokenize(fr_text)\n\nsource = []\ntarget = []\nfor en_sent, fr_sent in zip(en_sentences, fr_sentences):\n    source.append([tok.text for tok in spacy_en.tokenizer(en_sent)])\n    target.append([tok.text for tok in spacy_fr.tokenizer(fr_sent)])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:23:59.967142Z","iopub.execute_input":"2022-04-12T17:23:59.967385Z","iopub.status.idle":"2022-04-12T17:24:13.428715Z","shell.execute_reply.started":"2022-04-12T17:23:59.967351Z","shell.execute_reply":"2022-04-12T17:24:13.427997Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# calculating frequency and assigning unknown to words having frequcny less than threshold\nen_freq = {}\nfr_freq = {}\nfor en_lst, fr_lst in zip(source, target):\n    for en_token in en_lst:\n        if en_freq.get(en_token) == None:\n            en_freq[en_token] = 1\n        else:\n            en_freq[en_token] += 1\n            \n    for fr_token in fr_lst:\n        if fr_freq.get(fr_token) == None:\n            fr_freq[fr_token] = 1\n        else:\n            fr_freq[fr_token] += 1\n            \n            \nen_sentences = []\nfr_sentences = []\nfor en_lst, fr_lst in zip(source, target):\n    en_sent = []\n    for word in en_lst:\n        if word not in string.punctuation:\n            if en_freq[word] >= threshold:\n                en_sent.append(word)\n            else:\n                en_sent.append('unk')\n                \n    en_sentences.append(en_sent)\n    \n    fr_sent = []\n    for word in fr_lst:\n        if word not in string.punctuation:\n            if fr_freq[word] >= threshold:\n                fr_sent.append(word)\n            else:\n                fr_sent.append('unk')\n        \n    fr_sentences.append(fr_sent)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:13.430050Z","iopub.execute_input":"2022-04-12T17:24:13.430293Z","iopub.status.idle":"2022-04-12T17:24:14.428273Z","shell.execute_reply.started":"2022-04-12T17:24:13.430261Z","shell.execute_reply":"2022-04-12T17:24:14.427537Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# storing unique words in vocabulary and creatinf sentecne of max_length and appending eos,sos tokens and creating word2index and index2word dicts\nen_vocab = set()\nfr_vocab = set()\npads = ['sos', 'eos', 'unk']\nfor p in pads:\n    en_vocab.add(p)\n    fr_vocab.add(p)\n\n# here we are storing words in src and trg\nsrc = []\ntrg = []\nfor en_sent, fr_sent in zip(en_sentences, fr_sentences):\n    en_lst = []\n    for i in range(min(max_length, len(en_sent))):\n        en_lst.append(en_sent[i])\n        en_vocab.add(en_sent[i])\n    \n    fr_lst = []\n    for i in range(min(max_length, len(fr_sent))):\n        fr_lst.append(fr_sent[i])\n        fr_vocab.add(fr_sent[i])\n    \n    src.append(en_lst)\n    trg.append(fr_lst)\n    \n\nen_token2index = {}\nen_index2token = {}\nfor cnt,token in enumerate(en_vocab):\n    en_token2index[token] = cnt\n    en_index2token[cnt] = token\n\nfr_token2index = {}\nfr_index2token = {}\nfor cnt,token in enumerate(fr_vocab):\n    fr_token2index[token] = cnt\n    fr_index2token[cnt] = token","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:14.429584Z","iopub.execute_input":"2022-04-12T17:24:14.429818Z","iopub.status.idle":"2022-04-12T17:24:14.857614Z","shell.execute_reply.started":"2022-04-12T17:24:14.429787Z","shell.execute_reply":"2022-04-12T17:24:14.856850Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"joblib.dump(en_token2index, 'en_token2index_q2.1.pkl')\njoblib.dump(fr_token2index, 'fr_token2index_q2.1.pkl')\njoblib.dump(en_index2token, 'en_index2token_q2.1.pkl')\njoblib.dump(fr_index2token, 'fr_index2token_q2.1.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:14.858797Z","iopub.execute_input":"2022-04-12T17:24:14.859138Z","iopub.status.idle":"2022-04-12T17:24:15.335338Z","shell.execute_reply.started":"2022-04-12T17:24:14.859100Z","shell.execute_reply":"2022-04-12T17:24:15.334552Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['fr_index2token_q2.1.pkl']"},"metadata":{}}]},{"cell_type":"code","source":"source = []\ntarget = []\npairs = []\nfor en_sent, fr_sent in zip(src, trg):\n    en_lst = []\n    for en_token in en_sent:\n        en_lst.append(en_token2index[en_token])\n    \n    fr_lst = []\n    for fr_token in fr_sent:\n        fr_lst.append(fr_token2index[fr_token])\n    \n    source.append(en_lst)\n    target.append(fr_lst)\n    pairs.append([en_lst,fr_lst])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:15.336867Z","iopub.execute_input":"2022-04-12T17:24:15.337133Z","iopub.status.idle":"2022-04-12T17:24:15.620475Z","shell.execute_reply.started":"2022-04-12T17:24:15.337098Z","shell.execute_reply":"2022-04-12T17:24:15.619724Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(len(src),len(trg),len(pairs))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:15.623300Z","iopub.execute_input":"2022-04-12T17:24:15.623588Z","iopub.status.idle":"2022-04-12T17:24:15.628923Z","shell.execute_reply.started":"2022-04-12T17:24:15.623550Z","shell.execute_reply":"2022-04-12T17:24:15.627868Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"30964 30964 30964\n","output_type":"stream"}]},{"cell_type":"code","source":"# ENCODER DECODER SEQ2SEQ\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n        super(Encoder, self).__init__()\n\n        #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n        self.input_dim = input_dim\n        self.embbed_dim = embbed_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        #initialize the embedding layer with input and embbed dimention\n        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n        #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n        #set the number of gru layers\n        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n              \n    def forward(self, src):\n\n        embedded = self.embedding(src).view(1,1,-1)\n        outputs, hidden = self.gru(embedded)\n        return outputs, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n        super(Decoder, self).__init__()\n\n        #set the encoder output dimension, embed dimension, hidden dimension, and number of layers \n        self.embbed_dim = embbed_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n\n        # initialize every layer with the appropriate dimension. For the decoder layer, it will consist of an embedding, GRU, a Linear layer and a Log softmax activation function.\n        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n        self.out = nn.Linear(self.hidden_dim, output_dim)\n        self.softmax = nn.LogSoftmax(dim=1)\n      \n    def forward(self, input, hidden):\n        # reshape the input to (1, batch_size)\n        input = input.view(1, -1)\n        embedded = F.relu(self.embedding(input))\n        output, hidden = self.gru(embedded, hidden)       \n        prediction = self.softmax(self.out(output[0]))\n\n        return prediction, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device, MAX_LENGTH=max_length):\n        super().__init__()\n      \n        #initialize the encoder and decoder\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        \n        input_length = source.size(0) #get the input length (number of words in sentence)\n        batch_size = target.shape[1] \n        target_length = target.shape[0]\n        vocab_size = self.decoder.output_dim\n\n        #initialize a variable to hold the predicted outputs\n        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n\n        #encode every word in a sentence\n        for i in range(input_length):\n            encoder_output, encoder_hidden = self.encoder(source[i])\n\n        #use the encoder’s hidden layer as the decoder hidden\n        decoder_hidden = encoder_hidden.to(device)\n\n        #add a token before the first predicted word\n        decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n\n        #topk is used to get the top K value over a list\n        #predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n\n        for t in range(target_length):   \n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n            outputs[t] = decoder_output\n            teacher_force = random.random() < teacher_forcing_ratio\n            topv, topi = decoder_output.topk(1)\n            input = (target[t] if teacher_force else topi)\n            if(teacher_force == False and input.item() == EOS_token):\n                break\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:15.630399Z","iopub.execute_input":"2022-04-12T17:24:15.631130Z","iopub.status.idle":"2022-04-12T17:24:15.650539Z","shell.execute_reply.started":"2022-04-12T17:24:15.631075Z","shell.execute_reply":"2022-04-12T17:24:15.649831Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def calcError(model, input_tensor, target_tensor, model_optimizer, criterion):\n    model_optimizer.zero_grad()\n\n    loss = 0\n    epoch_loss = 0\n\n    output = model(input_tensor, target_tensor)\n\n    num_iter = output.size(0)\n                \n    #calculate the loss from a predicted sentence with the expected result\n    for ot in range(num_iter):\n        loss += criterion(output[ot], target_tensor[ot])\n\n    loss.backward()\n    model_optimizer.step()\n    epoch_loss = loss.item() / num_iter\n    return epoch_loss\n\n\ndef trainModel(model, pairs, num_iteration):\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n    criterion = nn.NLLLoss()\n    total_loss_iterations = 0\n\n    for iter in range(1, num_iteration+1):\n        training_pair = pairs[iter - 1]\n        input_tensor = torch.tensor(training_pair[0], dtype=torch.long, device=device).view(-1, 1)\n        target_tensor = torch.tensor(training_pair[1], dtype=torch.long, device=device).view(-1, 1)\n        \n        try:\n            loss = calcError(model, input_tensor, target_tensor, optimizer, criterion)\n        except:\n            do_nothing = 1\n        \n        total_loss_iterations += loss\n\n        if iter % 5000 == 0:\n            avarage_loss= total_loss_iterations / 5000\n            total_loss_iterations = 0\n            print('Iteration Number: %d     Avg Loss: %.4f' % (iter, avarage_loss))\n          \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:15.651982Z","iopub.execute_input":"2022-04-12T17:24:15.652266Z","iopub.status.idle":"2022-04-12T17:24:15.664401Z","shell.execute_reply.started":"2022-04-12T17:24:15.652202Z","shell.execute_reply":"2022-04-12T17:24:15.663693Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"input_size = len(en_vocab)\noutput_size = len(fr_vocab)\nprint('Input : {} Output : {}'.format(input_size, output_size))\n\nembed_size = 256\nhidden_size = 512\nnum_layers = 1\n# total number of senetences\nnum_iteration = len(pairs)\n\n#create encoder-decoder model\nencoder = Encoder(input_size, hidden_size, embed_size, num_layers)\ndecoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n\nmodel = Seq2Seq(encoder, decoder, device).to(device)\n\n#print model \nprint(encoder)\nprint(decoder)\n\nfor ep in range(epochs):\n    model = trainModel(model, pairs, num_iteration)\n    print('Epoch {} done'.format(ep+1))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:24:15.665857Z","iopub.execute_input":"2022-04-12T17:24:15.666346Z","iopub.status.idle":"2022-04-12T18:24:32.036324Z","shell.execute_reply.started":"2022-04-12T17:24:15.666311Z","shell.execute_reply":"2022-04-12T18:24:32.035421Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Input : 12991 Output : 15798\nEncoder(\n  (embedding): Embedding(12991, 256)\n  (gru): GRU(256, 512)\n)\nDecoder(\n  (embedding): Embedding(15798, 256)\n  (gru): GRU(256, 512)\n  (out): Linear(in_features=512, out_features=15798, bias=True)\n  (softmax): LogSoftmax(dim=1)\n)\nIteration Number: 5000     Avg Loss: 6.6665\nIteration Number: 10000     Avg Loss: 6.5253\nIteration Number: 15000     Avg Loss: 6.5179\nIteration Number: 20000     Avg Loss: 6.5206\nIteration Number: 25000     Avg Loss: 6.4710\nIteration Number: 30000     Avg Loss: 6.5053\nEpoch 1 done\nIteration Number: 5000     Avg Loss: 6.3945\nIteration Number: 10000     Avg Loss: 6.3771\nIteration Number: 15000     Avg Loss: 6.3945\nIteration Number: 20000     Avg Loss: 6.3915\nIteration Number: 25000     Avg Loss: 6.3616\nIteration Number: 30000     Avg Loss: 6.4022\nEpoch 2 done\nIteration Number: 5000     Avg Loss: 6.2997\nIteration Number: 10000     Avg Loss: 6.2845\nIteration Number: 15000     Avg Loss: 6.3058\nIteration Number: 20000     Avg Loss: 6.2896\nIteration Number: 25000     Avg Loss: 6.2769\nIteration Number: 30000     Avg Loss: 6.3163\nEpoch 3 done\nIteration Number: 5000     Avg Loss: 6.2272\nIteration Number: 10000     Avg Loss: 6.2100\nIteration Number: 15000     Avg Loss: 6.2274\nIteration Number: 20000     Avg Loss: 6.2020\nIteration Number: 25000     Avg Loss: 6.1926\nIteration Number: 30000     Avg Loss: 6.2250\nEpoch 4 done\nIteration Number: 5000     Avg Loss: 6.1486\nIteration Number: 10000     Avg Loss: 6.1280\nIteration Number: 15000     Avg Loss: 6.1393\nIteration Number: 20000     Avg Loss: 6.1096\nIteration Number: 25000     Avg Loss: 6.1027\nIteration Number: 30000     Avg Loss: 6.1276\nEpoch 5 done\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'q2.1_train.pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:24:32.040804Z","iopub.execute_input":"2022-04-12T18:24:32.041153Z","iopub.status.idle":"2022-04-12T18:24:32.253081Z","shell.execute_reply.started":"2022-04-12T18:24:32.041115Z","shell.execute_reply":"2022-04-12T18:24:32.252233Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def french_sentence(input_sentence):\n    input_tensor = torch.tensor(input_sentence, dtype=torch.long, device=device).view(-1, 1)\n    target = np.zeros(len(input_sentence))\n    output_tensor = torch.tensor(target, dtype=torch.long, device=device).view(-1, 1)\n    # setting the teacher forcing ratio to be 0.0, so we get only the words predicted by the model     \n    output = model(input_tensor, output_tensor,0.0)\n    num_iter = output.size(0)\n    decoded_words = []\n    for ot in range(output.size(0)):\n        topv, topi = output[ot].topk(1)\n        # print(topi)\n\n        if topi[0].item() == EOS_token:\n            decoded_words.append('<EOS>')\n            break\n        else:\n            decoded_words.append(fr_index2token[topi[0].item()])\n\n    return decoded_words","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:24:32.271057Z","iopub.execute_input":"2022-04-12T18:24:32.271669Z","iopub.status.idle":"2022-04-12T18:24:32.288392Z","shell.execute_reply.started":"2022-04-12T18:24:32.271632Z","shell.execute_reply":"2022-04-12T18:24:32.287629Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def write_in_file(file_name, trg, pairs):\n    corpus_references = []\n    corpus_candidates = []\n\n    sentence_score = []\n    length = len(trg)\n    french = []\n    for i in range(length):\n        reference = trg[i]\n        french.append(french_sentence(pairs[i][0]))\n        candidate = french[i]\n        try:\n            score = sentence_bleu(reference, candidate)\n        except:\n            score = 0.0\n        sentence_score.append(score)\n        corpus_references.append(reference)\n        corpus_candidates.append(candidate)\n\n    corpus_score = corpus_bleu(corpus_references, corpus_candidates)  \n    \n    to_write = ''\n    to_write += (str(corpus_score) + '\\n')\n    \n    for cnt, lst in enumerate(french):\n        sent = \" \".join(lst)\n        to_write += sent\n        to_write += '     '\n        to_write += str(sentence_score[cnt])\n        to_write += '\\n'\n        \n    file = open(file_name, 'w')\n    file.write(to_write)\n    file.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:17:29.841379Z","iopub.execute_input":"2022-04-12T19:17:29.842058Z","iopub.status.idle":"2022-04-12T19:17:29.851422Z","shell.execute_reply.started":"2022-04-12T19:17:29.842018Z","shell.execute_reply":"2022-04-12T19:17:29.848846Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"write_in_file('2019101056_MT1_train.txt',trg,pairs)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:17:30.651188Z","iopub.execute_input":"2022-04-12T19:17:30.651452Z","iopub.status.idle":"2022-04-12T19:27:42.812857Z","shell.execute_reply.started":"2022-04-12T19:17:30.651413Z","shell.execute_reply":"2022-04-12T19:27:42.812111Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def generate_for_test(english_filename, french_filename):\n    # read file\n    f = open(english_file_name)\n    en_text = f.read()\n    f.close()\n\n    f = open(french_file_name)\n    fr_text = f.read()\n    f.close()\n\n    # tokenize french and english language sentences\n    en_text = en_text.lower()\n    en_sentences = nltk.tokenize.sent_tokenize(en_text)\n\n    fr_text = fr_text.lower()\n    fr_sentences = nltk.tokenize.sent_tokenize(fr_text)\n\n    source = []\n    target = []\n    for en_sent, fr_sent in zip(en_sentences, fr_sentences):\n        source.append([tok.text for tok in spacy_en.tokenizer(en_sent)])\n        target.append([tok.text for tok in spacy_fr.tokenizer(fr_sent)])\n\n\n    trg = []\n    pairs = []\n    for en_sent, fr_sent in zip(source,target):\n        en_lst = []\n        for token in en_sent:\n            if en_token2index.get(token) == None:\n                en_lst.append(en_token2index['unk'])\n            else:\n                en_lst.append(en_token2index[token])\n\n\n        fr_lst = []\n        fr_tokens = []\n        for token in fr_sent:\n            if fr_token2index.get(token) == None:\n                fr_lst.append(fr_token2index['unk'])\n                fr_tokens.append('unk')\n            else:\n                fr_lst.append(fr_token2index[token])\n                fr_tokens.append(token)\n\n        trg.append(fr_tokens)\n        pairs.append([en_lst,fr_lst])\n        \n    return trg, pairs","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:27:42.814638Z","iopub.execute_input":"2022-04-12T19:27:42.814898Z","iopub.status.idle":"2022-04-12T19:27:42.826046Z","shell.execute_reply.started":"2022-04-12T19:27:42.814860Z","shell.execute_reply":"2022-04-12T19:27:42.825331Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# FOR GENERATING FILE ON TEST DATASET\n\nenglish_filename = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/test.en'\nfrench_filename = '../input/dataset/intro-to-nlp-assign3/ted-talks-corpus/test.fr'\ntrg, pairs = generate_for_test(english_filename, french_filename)\nwrite_in_file('2019101056_MT1_test.txt',trg,pairs)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:27:42.827383Z","iopub.execute_input":"2022-04-12T19:27:42.828014Z","iopub.status.idle":"2022-04-12T19:38:02.224438Z","shell.execute_reply.started":"2022-04-12T19:27:42.827976Z","shell.execute_reply":"2022-04-12T19:38:02.223719Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}